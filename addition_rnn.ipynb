{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "addition_rnn",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJ1rebQqJ-p",
        "colab_type": "text"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "# 개요\n",
        "\n",
        "- 원 본 : https://keras.io/examples/nlp/addition_rnn/\n",
        "- 작업 : 문자열 매핑. 덧셈\n",
        "- 데이터 : 생성한 데이터\n",
        "- 적용 모델 : LSTM\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# 작업 예\n",
        "\n",
        "```\n",
        "입력문자열 ---> 출력문자열\n",
        "'   81+7'  --->  '25  '\n",
        "'  2+385'  --->  '585 '\n",
        "'  21+85'  --->  '70  '\n",
        "'   58+5'  --->  '90  '\n",
        "'  683+8'  --->  '394 '\n",
        "'   04+9'  --->  '49  '\n",
        "'    5+3'  --->  '8   '\n",
        "' 26+334'  --->  '495 '\n",
        "'  198+7'  --->  '898 '\n",
        "'   48+1'  --->  '85  '\n",
        "```\n",
        "\n",
        "문자열을 파싱하여 덧셈을 수행하고 문자열을 출력\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "voPEVl5go-Ea"
      },
      "source": [
        "# Sequence to sequence learning for performing number addition\n",
        "\n",
        "**Author:** [Smerity](https://twitter.com/Smerity) and others<br>\n",
        "**Date created:** 2015/08/17<br>\n",
        "**Last modified:** 2020/04/17<br>\n",
        "**Description:** A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0eHMjgzSo-Eb"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we train a model to learn to add two numbers, provided as strings.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Input: \"535+61\"\n",
        "- Output: \"596\"\n",
        "\n",
        "Input may optionally be reversed, which was shown to increase performance in many tasks\n",
        " in: [Learning to Execute](http://arxiv.org/abs/1410.4615) and\n",
        "[Sequence to Sequence Learning with Neural Networks](\n",
        "\n",
        " http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
        "\n",
        "Theoretically, sequence order inversion introduces shorter term dependencies between\n",
        " source and target for this problem.\n",
        "\n",
        "**Results:**\n",
        "\n",
        "For two digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits (reversed):\n",
        "\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ou-ygxNfo-Ec"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M5OvBIiWo-Ec",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dQt0G10o-Eg"
      },
      "source": [
        "## Generate the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COsdmY5vo-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8ca46476-06fd-4f8a-fbfe-4ea198c5597b"
      },
      "source": [
        "\n",
        "class CharacterTable:\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjb53hlWqQB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "cf6cd8ff-4a05-4115-e366-40fd181a407c"
      },
      "source": [
        "for q, a in zip(questions[:10], expected[:10]):\n",
        "  print(\"q='{}', e='{}'\".format(q, a))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q='   81+7', e='25  '\n",
            "q='  2+385', e='585 '\n",
            "q='  21+85', e='70  '\n",
            "q='   58+5', e='90  '\n",
            "q='  683+8', e='394 '\n",
            "q='   04+9', e='49  '\n",
            "q='    5+3', e='8   '\n",
            "q=' 26+334', e='495 '\n",
            "q='  198+7', e='898 '\n",
            "q='   48+1', e='85  '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFNcwxPOo-Ej"
      },
      "source": [
        "## Vectorize the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QrdLQKGra06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80ddc7b3-9019-4ab2-a9d8-73f9ca3dc151"
      },
      "source": [
        "print(chars)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789+ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwm40AA-r4tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "9d49189c-127d-462e-8234-e8542b7f840b"
      },
      "source": [
        "encoded = ctable.encode(' 123+45', MAXLEN)\n",
        "# [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- ' '\n",
        "#  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- '1'\n",
        "#  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]      <--- '2'\n",
        "#  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]      <--- '3'\n",
        "#  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- '+'\n",
        "#  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]      <--- '4'\n",
        "#  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]     <--- '5'\n",
        "print(encoded)\n",
        "print(encoded.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "(7, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d1QHj3MDo-Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "9bc6dab2-f815-41b3-d204-ad6061329d10"
      },
      "source": [
        "print(\"Vectorization...\")\n",
        "\n",
        "# DIGITS = 3\n",
        "# MAXLEN = DIGITS + 1 + DIGITS\n",
        "# chars = '0123456789+'\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "# x 데이터 한개의 모양은 (7, 12)  # 7 글자, 12개 글자 종류(0~9, '+', ' ')\n",
        "# y 데이터 한개의 모양은 (3, 12)  # 3 글자, 12개 글자 종류(0~9, '+', ' ')\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2liXf2Yo-En"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP8Vw2aNtzPn",
        "colab_type": "text"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "LSTM 레이어의 출력은 128 차원의 벡터\n",
        "\n",
        "```\n",
        "LSTM            (None, 128)\n",
        "RepeatVector    (None, 4, 128)    # [1,2] -> [ [1,2], [1,2], [1,2], [1,2] ]\n",
        "LSTM            (None, 4, 128)\n",
        "Dense           (None, 4, 12)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GUr-sqkoo-En",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "aeafbfdd-2b0b-424c-bdec-01d0f65b63a7"
      },
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(num_layers):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N5L-7r5Yo-Er"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-Zav9Fro-Er",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b40bbae-7c35-4221-c8ad-5c104cd2f517"
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        if correct == guess:\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7597 - accuracy: 0.3562 - val_loss: 1.5493 - val_accuracy: 0.4158\n",
            "Q 39+360  T 399  ☒ 368 \n",
            "Q 55+230  T 285  ☒ 568 \n",
            "Q 948+8   T 956  ☒ 906 \n",
            "Q 8+108   T 116  ☒ 122 \n",
            "Q 226+96  T 322  ☑ 322 \n",
            "Q 549+160 T 709  ☒ 601 \n",
            "Q 521+40  T 561  ☒ 566 \n",
            "Q 33+80   T 113  ☒ 33  \n",
            "Q 39+735  T 774  ☒ 422 \n",
            "Q 140+933 T 1073 ☒ 1211\n",
            "\n",
            "Iteration 2\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3196 - accuracy: 0.5081 - val_loss: 1.1371 - val_accuracy: 0.5782\n",
            "Q 912+977 T 1889 ☒ 1807\n",
            "Q 73+674  T 747  ☒ 752 \n",
            "Q 2+574   T 576  ☒ 572 \n",
            "Q 83+952  T 1035 ☒ 1021\n",
            "Q 770+36  T 806  ☒ 811 \n",
            "Q 607+86  T 693  ☒ 756 \n",
            "Q 83+453  T 536  ☒ 522 \n",
            "Q 629+58  T 687  ☒ 682 \n",
            "Q 131+62  T 193  ☒ 192 \n",
            "Q 88+253  T 341  ☒ 322 \n",
            "\n",
            "Iteration 3\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0168 - accuracy: 0.6248 - val_loss: 0.8989 - val_accuracy: 0.6695\n",
            "Q 35+407  T 442  ☒ 441 \n",
            "Q 15+133  T 148  ☒ 150 \n",
            "Q 2+867   T 869  ☒ 873 \n",
            "Q 8+601   T 609  ☒ 616 \n",
            "Q 767+268 T 1035 ☒ 1021\n",
            "Q 1+159   T 160  ☑ 160 \n",
            "Q 420+29  T 449  ☒ 459 \n",
            "Q 790+22  T 812  ☒ 819 \n",
            "Q 772+641 T 1413 ☒ 1412\n",
            "Q 0+146   T 146  ☒ 159 \n",
            "\n",
            "Iteration 4\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.8470 - accuracy: 0.6890 - val_loss: 0.7984 - val_accuracy: 0.7096\n",
            "Q 876+126 T 1002 ☒ 900 \n",
            "Q 43+772  T 815  ☑ 815 \n",
            "Q 96+863  T 959  ☒ 952 \n",
            "Q 76+198  T 274  ☒ 275 \n",
            "Q 25+533  T 558  ☒ 551 \n",
            "Q 994+145 T 1139 ☒ 1145\n",
            "Q 56+151  T 207  ☒ 219 \n",
            "Q 821+4   T 825  ☒ 826 \n",
            "Q 826+85  T 911  ☒ 910 \n",
            "Q 31+129  T 160  ☒ 165 \n",
            "\n",
            "Iteration 5\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7507 - accuracy: 0.7279 - val_loss: 0.7476 - val_accuracy: 0.7208\n",
            "Q 714+95  T 809  ☒ 815 \n",
            "Q 85+39   T 124  ☑ 124 \n",
            "Q 4+127   T 131  ☑ 131 \n",
            "Q 981+286 T 1267 ☑ 1267\n",
            "Q 860+1   T 861  ☒ 862 \n",
            "Q 187+851 T 1038 ☒ 1121\n",
            "Q 451+8   T 459  ☒ 469 \n",
            "Q 918+13  T 931  ☒ 930 \n",
            "Q 784+87  T 871  ☒ 863 \n",
            "Q 192+39  T 231  ☒ 232 \n",
            "\n",
            "Iteration 6\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6507 - accuracy: 0.7626 - val_loss: 0.5586 - val_accuracy: 0.7965\n",
            "Q 12+227  T 239  ☒ 247 \n",
            "Q 392+80  T 472  ☒ 474 \n",
            "Q 92+59   T 151  ☒ 150 \n",
            "Q 89+989  T 1078 ☒ 1075\n",
            "Q 824+748 T 1572 ☒ 1571\n",
            "Q 285+60  T 345  ☒ 333 \n",
            "Q 37+58   T 95   ☒ 94  \n",
            "Q 63+922  T 985  ☒ 986 \n",
            "Q 5+894   T 899  ☒ 900 \n",
            "Q 51+403  T 454  ☒ 453 \n",
            "\n",
            "Iteration 7\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.4075 - accuracy: 0.8593 - val_loss: 0.2966 - val_accuracy: 0.9067\n",
            "Q 551+2   T 553  ☒ 554 \n",
            "Q 75+619  T 694  ☒ 705 \n",
            "Q 942+24  T 966  ☑ 966 \n",
            "Q 99+95   T 194  ☒ 103 \n",
            "Q 89+774  T 863  ☑ 863 \n",
            "Q 11+4    T 15   ☑ 15  \n",
            "Q 450+81  T 531  ☒ 532 \n",
            "Q 567+705 T 1272 ☑ 1272\n",
            "Q 9+948   T 957  ☒ 965 \n",
            "Q 60+395  T 455  ☒ 454 \n",
            "\n",
            "Iteration 8\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.2181 - accuracy: 0.9413 - val_loss: 0.1621 - val_accuracy: 0.9589\n",
            "Q 27+849  T 876  ☑ 876 \n",
            "Q 776+699 T 1475 ☒ 1474\n",
            "Q 8+287   T 295  ☑ 295 \n",
            "Q 74+431  T 505  ☒ 405 \n",
            "Q 586+38  T 624  ☑ 624 \n",
            "Q 9+947   T 956  ☑ 956 \n",
            "Q 2+405   T 407  ☑ 407 \n",
            "Q 997+78  T 1075 ☒ 1065\n",
            "Q 7+559   T 566  ☑ 566 \n",
            "Q 44+221  T 265  ☑ 265 \n",
            "\n",
            "Iteration 9\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.1300 - accuracy: 0.9680 - val_loss: 0.1048 - val_accuracy: 0.9726\n",
            "Q 268+67  T 335  ☑ 335 \n",
            "Q 165+19  T 184  ☑ 184 \n",
            "Q 747+10  T 757  ☑ 757 \n",
            "Q 117+38  T 155  ☑ 155 \n",
            "Q 2+123   T 125  ☒ 124 \n",
            "Q 19+926  T 945  ☑ 945 \n",
            "Q 59+57   T 116  ☑ 116 \n",
            "Q 67+317  T 384  ☑ 384 \n",
            "Q 203+618 T 821  ☑ 821 \n",
            "Q 7+385   T 392  ☑ 392 \n",
            "\n",
            "Iteration 10\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0866 - accuracy: 0.9795 - val_loss: 0.0645 - val_accuracy: 0.9861\n",
            "Q 826+522 T 1348 ☑ 1348\n",
            "Q 242+97  T 339  ☑ 339 \n",
            "Q 39+241  T 280  ☑ 280 \n",
            "Q 111+30  T 141  ☑ 141 \n",
            "Q 857+85  T 942  ☑ 942 \n",
            "Q 942+64  T 1006 ☑ 1006\n",
            "Q 72+686  T 758  ☑ 758 \n",
            "Q 550+17  T 567  ☑ 567 \n",
            "Q 585+647 T 1232 ☑ 1232\n",
            "Q 40+708  T 748  ☑ 748 \n",
            "\n",
            "Iteration 11\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0745 - accuracy: 0.9803 - val_loss: 0.0570 - val_accuracy: 0.9851\n",
            "Q 613+2   T 615  ☑ 615 \n",
            "Q 3+847   T 850  ☑ 850 \n",
            "Q 24+258  T 282  ☑ 282 \n",
            "Q 99+323  T 422  ☑ 422 \n",
            "Q 83+28   T 111  ☑ 111 \n",
            "Q 510+848 T 1358 ☑ 1358\n",
            "Q 740+84  T 824  ☑ 824 \n",
            "Q 40+31   T 71   ☑ 71  \n",
            "Q 156+43  T 199  ☑ 199 \n",
            "Q 87+222  T 309  ☑ 309 \n",
            "\n",
            "Iteration 12\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0470 - accuracy: 0.9889 - val_loss: 0.2280 - val_accuracy: 0.9340\n",
            "Q 24+364  T 388  ☑ 388 \n",
            "Q 5+126   T 131  ☑ 131 \n",
            "Q 767+755 T 1522 ☑ 1522\n",
            "Q 65+231  T 296  ☑ 296 \n",
            "Q 495+48  T 543  ☑ 543 \n",
            "Q 617+649 T 1266 ☒ 1366\n",
            "Q 45+404  T 449  ☑ 449 \n",
            "Q 0+102   T 102  ☑ 102 \n",
            "Q 346+303 T 649  ☑ 649 \n",
            "Q 10+254  T 264  ☑ 264 \n",
            "\n",
            "Iteration 13\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0489 - accuracy: 0.9884 - val_loss: 0.0879 - val_accuracy: 0.9728\n",
            "Q 646+33  T 679  ☑ 679 \n",
            "Q 71+72   T 143  ☑ 143 \n",
            "Q 831+53  T 884  ☑ 884 \n",
            "Q 44+66   T 110  ☑ 110 \n",
            "Q 667+23  T 690  ☑ 690 \n",
            "Q 27+951  T 978  ☑ 978 \n",
            "Q 741+663 T 1404 ☑ 1404\n",
            "Q 7+132   T 139  ☑ 139 \n",
            "Q 598+132 T 730  ☑ 730 \n",
            "Q 9+626   T 635  ☒ 636 \n",
            "\n",
            "Iteration 14\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0551 - accuracy: 0.9849 - val_loss: 0.0644 - val_accuracy: 0.9837\n",
            "Q 83+792  T 875  ☑ 875 \n",
            "Q 4+689   T 693  ☑ 693 \n",
            "Q 593+141 T 734  ☑ 734 \n",
            "Q 193+419 T 612  ☑ 612 \n",
            "Q 4+896   T 900  ☑ 900 \n",
            "Q 228+590 T 818  ☑ 818 \n",
            "Q 597+47  T 644  ☑ 644 \n",
            "Q 681+79  T 760  ☑ 760 \n",
            "Q 99+323  T 422  ☑ 422 \n",
            "Q 83+41   T 124  ☑ 124 \n",
            "\n",
            "Iteration 15\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0362 - accuracy: 0.9904 - val_loss: 0.1076 - val_accuracy: 0.9622\n",
            "Q 951+592 T 1543 ☒ 1553\n",
            "Q 2+247   T 249  ☑ 249 \n",
            "Q 456+68  T 524  ☑ 524 \n",
            "Q 30+610  T 640  ☑ 640 \n",
            "Q 834+394 T 1228 ☑ 1228\n",
            "Q 716+52  T 768  ☑ 768 \n",
            "Q 80+42   T 122  ☑ 122 \n",
            "Q 91+29   T 120  ☒ 110 \n",
            "Q 98+956  T 1054 ☑ 1054\n",
            "Q 774+8   T 782  ☑ 782 \n",
            "\n",
            "Iteration 16\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0367 - accuracy: 0.9901 - val_loss: 0.0184 - val_accuracy: 0.9965\n",
            "Q 771+960 T 1731 ☑ 1731\n",
            "Q 503+503 T 1006 ☑ 1006\n",
            "Q 205+0   T 205  ☑ 205 \n",
            "Q 35+407  T 442  ☑ 442 \n",
            "Q 604+7   T 611  ☑ 611 \n",
            "Q 24+136  T 160  ☑ 160 \n",
            "Q 48+342  T 390  ☑ 390 \n",
            "Q 33+89   T 122  ☑ 122 \n",
            "Q 857+673 T 1530 ☑ 1530\n",
            "Q 98+791  T 889  ☑ 889 \n",
            "\n",
            "Iteration 17\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.0213 - val_accuracy: 0.9944\n",
            "Q 387+21  T 408  ☑ 408 \n",
            "Q 524+0   T 524  ☑ 524 \n",
            "Q 87+303  T 390  ☑ 390 \n",
            "Q 716+52  T 768  ☑ 768 \n",
            "Q 388+28  T 416  ☑ 416 \n",
            "Q 406+794 T 1200 ☒ 1290\n",
            "Q 48+84   T 132  ☑ 132 \n",
            "Q 930+68  T 998  ☑ 998 \n",
            "Q 10+355  T 365  ☑ 365 \n",
            "Q 741+663 T 1404 ☑ 1404\n",
            "\n",
            "Iteration 18\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0341 - accuracy: 0.9904 - val_loss: 0.0300 - val_accuracy: 0.9906\n",
            "Q 399+41  T 440  ☑ 440 \n",
            "Q 295+31  T 326  ☑ 326 \n",
            "Q 46+71   T 117  ☑ 117 \n",
            "Q 289+746 T 1035 ☑ 1035\n",
            "Q 732+74  T 806  ☑ 806 \n",
            "Q 51+98   T 149  ☑ 149 \n",
            "Q 4+788   T 792  ☑ 792 \n",
            "Q 294+2   T 296  ☑ 296 \n",
            "Q 129+200 T 329  ☒ 339 \n",
            "Q 377+743 T 1120 ☑ 1120\n",
            "\n",
            "Iteration 19\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0131 - accuracy: 0.9973 - val_loss: 0.0231 - val_accuracy: 0.9929\n",
            "Q 938+48  T 986  ☑ 986 \n",
            "Q 96+921  T 1017 ☑ 1017\n",
            "Q 8+552   T 560  ☑ 560 \n",
            "Q 989+63  T 1052 ☑ 1052\n",
            "Q 5+614   T 619  ☑ 619 \n",
            "Q 701+139 T 840  ☒ 830 \n",
            "Q 549+54  T 603  ☑ 603 \n",
            "Q 632+91  T 723  ☑ 723 \n",
            "Q 954+627 T 1581 ☑ 1581\n",
            "Q 17+862  T 879  ☑ 879 \n",
            "\n",
            "Iteration 20\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.0194 - val_accuracy: 0.9944\n",
            "Q 74+810  T 884  ☑ 884 \n",
            "Q 984+209 T 1193 ☑ 1193\n",
            "Q 255+20  T 275  ☑ 275 \n",
            "Q 63+718  T 781  ☑ 781 \n",
            "Q 452+40  T 492  ☑ 492 \n",
            "Q 815+62  T 877  ☑ 877 \n",
            "Q 61+35   T 96   ☑ 96  \n",
            "Q 348+991 T 1339 ☑ 1339\n",
            "Q 319+29  T 348  ☑ 348 \n",
            "Q 42+145  T 187  ☑ 187 \n",
            "\n",
            "Iteration 21\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0277 - accuracy: 0.9927 - val_loss: 0.0409 - val_accuracy: 0.9873\n",
            "Q 879+13  T 892  ☑ 892 \n",
            "Q 931+68  T 999  ☒ 1009\n",
            "Q 34+819  T 853  ☑ 853 \n",
            "Q 15+96   T 111  ☑ 111 \n",
            "Q 90+183  T 273  ☑ 273 \n",
            "Q 396+214 T 610  ☑ 610 \n",
            "Q 940+42  T 982  ☑ 982 \n",
            "Q 567+100 T 667  ☑ 667 \n",
            "Q 876+20  T 896  ☑ 896 \n",
            "Q 92+822  T 914  ☑ 914 \n",
            "\n",
            "Iteration 22\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0200 - accuracy: 0.9944 - val_loss: 0.0303 - val_accuracy: 0.9905\n",
            "Q 713+353 T 1066 ☑ 1066\n",
            "Q 523+969 T 1492 ☑ 1492\n",
            "Q 431+559 T 990  ☑ 990 \n",
            "Q 753+968 T 1721 ☑ 1721\n",
            "Q 160+90  T 250  ☑ 250 \n",
            "Q 652+778 T 1430 ☑ 1430\n",
            "Q 78+428  T 506  ☑ 506 \n",
            "Q 69+65   T 134  ☑ 134 \n",
            "Q 283+954 T 1237 ☑ 1237\n",
            "Q 885+117 T 1002 ☑ 1002\n",
            "\n",
            "Iteration 23\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0261 - accuracy: 0.9925 - val_loss: 0.0195 - val_accuracy: 0.9944\n",
            "Q 87+55   T 142  ☑ 142 \n",
            "Q 70+641  T 711  ☑ 711 \n",
            "Q 619+155 T 774  ☑ 774 \n",
            "Q 497+64  T 561  ☑ 561 \n",
            "Q 81+437  T 518  ☑ 518 \n",
            "Q 5+257   T 262  ☑ 262 \n",
            "Q 172+95  T 267  ☑ 267 \n",
            "Q 479+826 T 1305 ☑ 1305\n",
            "Q 593+47  T 640  ☑ 640 \n",
            "Q 496+5   T 501  ☑ 501 \n",
            "\n",
            "Iteration 24\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0375 - val_accuracy: 0.9880\n",
            "Q 675+928 T 1603 ☑ 1603\n",
            "Q 2+363   T 365  ☑ 365 \n",
            "Q 15+449  T 464  ☑ 464 \n",
            "Q 290+7   T 297  ☑ 297 \n",
            "Q 106+17  T 123  ☑ 123 \n",
            "Q 301+809 T 1110 ☑ 1110\n",
            "Q 226+96  T 322  ☑ 322 \n",
            "Q 791+55  T 846  ☑ 846 \n",
            "Q 662+30  T 692  ☑ 692 \n",
            "Q 933+376 T 1309 ☑ 1309\n",
            "\n",
            "Iteration 25\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0212 - accuracy: 0.9942 - val_loss: 0.0512 - val_accuracy: 0.9817\n",
            "Q 382+46  T 428  ☑ 428 \n",
            "Q 564+93  T 657  ☑ 657 \n",
            "Q 47+36   T 83   ☑ 83  \n",
            "Q 31+221  T 252  ☑ 252 \n",
            "Q 96+558  T 654  ☒ 644 \n",
            "Q 767+268 T 1035 ☑ 1035\n",
            "Q 15+449  T 464  ☑ 464 \n",
            "Q 74+431  T 505  ☑ 505 \n",
            "Q 815+16  T 831  ☑ 831 \n",
            "Q 160+90  T 250  ☑ 250 \n",
            "\n",
            "Iteration 26\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0142 - accuracy: 0.9961 - val_loss: 0.0135 - val_accuracy: 0.9964\n",
            "Q 594+688 T 1282 ☑ 1282\n",
            "Q 977+908 T 1885 ☑ 1885\n",
            "Q 2+896   T 898  ☑ 898 \n",
            "Q 73+348  T 421  ☑ 421 \n",
            "Q 8+807   T 815  ☑ 815 \n",
            "Q 8+652   T 660  ☑ 660 \n",
            "Q 58+294  T 352  ☑ 352 \n",
            "Q 366+145 T 511  ☑ 511 \n",
            "Q 55+69   T 124  ☑ 124 \n",
            "Q 69+456  T 525  ☑ 525 \n",
            "\n",
            "Iteration 27\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0227 - accuracy: 0.9936 - val_loss: 0.0060 - val_accuracy: 0.9987\n",
            "Q 782+19  T 801  ☑ 801 \n",
            "Q 79+97   T 176  ☑ 176 \n",
            "Q 2+687   T 689  ☑ 689 \n",
            "Q 735+962 T 1697 ☑ 1697\n",
            "Q 228+83  T 311  ☑ 311 \n",
            "Q 15+361  T 376  ☑ 376 \n",
            "Q 82+471  T 553  ☑ 553 \n",
            "Q 874+94  T 968  ☑ 968 \n",
            "Q 544+23  T 567  ☑ 567 \n",
            "Q 74+810  T 884  ☑ 884 \n",
            "\n",
            "Iteration 28\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0158 - accuracy: 0.9957 - val_loss: 0.0046 - val_accuracy: 0.9992\n",
            "Q 21+753  T 774  ☑ 774 \n",
            "Q 492+45  T 537  ☑ 537 \n",
            "Q 91+66   T 157  ☑ 157 \n",
            "Q 294+6   T 300  ☑ 300 \n",
            "Q 952+499 T 1451 ☑ 1451\n",
            "Q 89+989  T 1078 ☑ 1078\n",
            "Q 283+430 T 713  ☑ 713 \n",
            "Q 799+483 T 1282 ☑ 1282\n",
            "Q 161+296 T 457  ☑ 457 \n",
            "Q 29+353  T 382  ☑ 382 \n",
            "\n",
            "Iteration 29\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0277 - accuracy: 0.9929 - val_loss: 0.0139 - val_accuracy: 0.9964\n",
            "Q 10+138  T 148  ☑ 148 \n",
            "Q 7+100   T 107  ☑ 107 \n",
            "Q 149+46  T 195  ☑ 195 \n",
            "Q 158+897 T 1055 ☑ 1055\n",
            "Q 792+559 T 1351 ☑ 1351\n",
            "Q 239+7   T 246  ☑ 246 \n",
            "Q 36+596  T 632  ☑ 632 \n",
            "Q 294+81  T 375  ☑ 375 \n",
            "Q 78+378  T 456  ☑ 456 \n",
            "Q 438+53  T 491  ☑ 491 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l3AO_JLlo-Eu"
      },
      "source": [
        "You'll get to 99+% validation accuracy after ~30 epochs.\n"
      ]
    }
  ]
}