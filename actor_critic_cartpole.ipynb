{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actor_critic_cartpole",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tsINHQW_VOm"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "# 개요\n",
        "\n",
        "- 원 본 : https://keras.io/examples/rl/actor_critic_cartpole/\n",
        "- 작업 : 강화학습으로 막대기 세우기\n",
        "- 데이터 : N/A\n",
        "- 적용 모델 : DNN\n",
        "- 적용 방법 : Actor Critic Method\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# 데이터\n",
        "\n",
        "N/A\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# 모델\n",
        "\n",
        "카트의 위치, 카트의 속도, 막대의 각도, 막대 맨위의 속도 4개를 입력으로 하고 \n",
        "좌나 우로 카드를 미는 2개읠 출력을 내는 DNN\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# OpenAI GYM CartPole-v0\n",
        "\n",
        "https://github.com/openai/gym/wiki/CartPole-v0\n",
        "\n",
        "Observation\n",
        "- 차 위치\n",
        "- 차 속도\n",
        "- 막대 각도\n",
        "- 막대 끝 속도\n",
        "\n",
        "Actions\n",
        "- 0 : 차를 우측으로 밀기\n",
        "- 1 : 차를 죄측으로 밀기\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# 강화학습\n",
        "\n",
        "다음을 반복하여 학습한다.\n",
        "1. action = model(state)\n",
        "1. sate, reward = env(action)\n",
        "1. reward로 model 업데이트\n",
        "\n",
        "\n",
        "\n",
        "# 전체 코드 흐름\n",
        "\n",
        "```\n",
        "while 성공할때 까지\n",
        "\n",
        "  while 넘어지거나 목표 스텝(1000)  <--- 요 루프 한번이 에피소드이다.\n",
        "\n",
        "    actor, critic = model(sate)\n",
        "    state, reward = env(action)\n",
        "\n",
        "    actor, critic, reward를 히스토리에 저장\n",
        "\n",
        "\n",
        "  critic 로스를 계산\n",
        "  action 로스를 계산\n",
        "\n",
        "  전체 로스 = critic 로스 + actor 로스\n",
        "\n",
        "  전체 로스를 가지고 model을 업데이트\n",
        "\n",
        "```\n",
        "\n",
        "actor와 reward만 있으면 될 것 같은데, critic이 왜 필요한지 모르겠다.\n",
        "\n",
        "아마도 그 이유가 Actor Critic Method의 핵심인 것 같다.\n",
        "\n",
        "이건 이론적으로 이해해야 알 것 같고.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6HRB4jY4wK-"
      },
      "source": [
        "# 태그\n",
        "\n",
        "```\n",
        "#reinforce_learning\n",
        "#cart_pole\n",
        "#dnn\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tAUvEMP-GCB"
      },
      "source": [
        "# Actor Critic Method\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/13<br>\n",
        "**Last modified:** 2020/05/13<br>\n",
        "**Description:** Implement Actor Critic Method in CartPole environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVDnVPV8-GCC"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This script shows an implementation of Actor Critic method on CartPole-V0 environment.\n",
        "\n",
        "### Actor Critic Method\n",
        "\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to two possible outputs:\n",
        "\n",
        "1. Recommended action: A probabiltiy value for each action in the action space.\n",
        "   The part of the agent responsible for this output is called the **actor**.\n",
        "2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n",
        "   future. The part of the agent responsible for this output is the **critic**.\n",
        "\n",
        "Agent and Critic learn to perform their tasks, such that the recommended actions\n",
        "from the actor maximize the rewards.\n",
        "\n",
        "### CartPole-V0\n",
        "\n",
        "A pole is attached to a cart placed on a frictionless track. The agent has to apply\n",
        "force to move the cart. It is rewarded for every time step the pole\n",
        "remains upright. The agent, therefore, must learn to keep the pole from falling over.\n",
        "\n",
        "### References\n",
        "\n",
        "- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maOdgEDJ-GCD"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0_4shGg-GCE"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration parameters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
        "env.seed(seed) # 재현을 위해.\n",
        "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58CVNpfZ-GCI"
      },
      "source": [
        "## Implement Actor Critic network\n",
        "\n",
        "This network learns two functions:\n",
        "\n",
        "1. Actor: This takes as input the state of our environment and returns a\n",
        "probability value for each action in its action space.\n",
        "2. Critic: This takes as input the state of our environment and returns\n",
        "an estimate of total rewards in the future.\n",
        "\n",
        "In our implementation, they share the initial layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0QYeHLvCKrO"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "observation 4개를 입력으로 하고 actor_probability와 critic_value 2개를 출력하는 DNN\n",
        "\n",
        "\n",
        "actor_probability는 [0,1, 0.9]의 모양. 0(좌측으로 밀기) 또는 1(우측으로 밀기)\n",
        "\n",
        "critic은 보상 값.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzHPiPcP-GCJ"
      },
      "source": [
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=[action, critic])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQIJXNdCd95"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "\n",
        "**용어**\n",
        "- episode : 한번 쭉 실행해 보는 것. 학습 하지 않고. 막대가 넘어지거나 최대 step이 지나면 종료된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RSma0PL-GCM"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zWvrS0--GCM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "46dd1b85-96d9-4864-ddef-42f1b52e5e77"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            # env.render(); Adding this line would show the attempts\n",
        "            # of the agent in a pop up window.\n",
        "\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            # from environment state\n",
        "            action_probs, critic_value = model(state)\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the past are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} falls after {:.0f} steps at episode {}\"\n",
        "        print(template.format(running_reward, episode_reward, episode_count))\n",
        "\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running reward: 14.47 falls after 25 steps at episode 10\n",
            "running reward: 35.84 falls after 152 steps at episode 20\n",
            "running reward: 76.86 falls after 145 steps at episode 30\n",
            "running reward: 102.82 falls after 164 steps at episode 40\n",
            "running reward: 111.63 falls after 42 steps at episode 50\n",
            "running reward: 128.15 falls after 127 steps at episode 60\n",
            "running reward: 141.36 falls after 200 steps at episode 70\n",
            "running reward: 164.89 falls after 200 steps at episode 80\n",
            "running reward: 172.59 falls after 200 steps at episode 90\n",
            "running reward: 179.35 falls after 200 steps at episode 100\n",
            "running reward: 183.70 falls after 200 steps at episode 110\n",
            "running reward: 183.97 falls after 193 steps at episode 120\n",
            "running reward: 190.20 falls after 200 steps at episode 130\n",
            "running reward: 193.51 falls after 200 steps at episode 140\n",
            "Solved at episode 146!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVqcs7rQP8En"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "영어 커멘트 삭제하고 이해한 내용을 커멘트로 달았다.\n",
        "\n",
        "변수명 살짝 바꿔주었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG-GBlh3CwNH"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = env.reset()\n",
        "    # sate = [-0.01258566 -0.00156614  0.04207708 -0.00180545]. 현재 상태 4개의 값.\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # 에피소드 1번을 쭉 실행한다.\n",
        "        # 실행하면서 critic_value, actor_probs, reward의 히스토리를 저장한다.\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            # state = [-0.01303166 -0.04270197 -0.04540492  0.04584364]\n",
        "            state = tf.expand_dims(state, 0)\n",
        "            # statue= [[-0.01303166 -0.04270197 -0.04540492  0.04584364]]\n",
        "\n",
        "            action_probs, critic_value = model(state)\n",
        "            # action_probs = [0.38199136 0.6180086 ]\n",
        "            # critic_value = [1.2710695]\n",
        "\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            # action = 0\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            # state = [ 0.04863056  0.17443249 -0.03863073 -0.31581318]\n",
        "            # reward = 1. 항상 1\n",
        "            # donw = False\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # 새로운 보상은 0.05의 비율로 추가하고, 기존의누적 보상은 0.95로 감소시킨다.\n",
        "        # 새 에피소드가 \n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "\n",
        "        # returns에 담긴 값은 뒤에서 critics의 레이블링 값으로 사용된다.\n",
        "        # 현 시점의 보상은 미래의 보상에 누적된다.\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "        # returns = [8.64827525163591, 7.72553055720799, 6.793465209301, 5.8519850599, 4.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0]\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "        # returns = [1.5310393742552764, 1.1572248237373701, 0.7796343686687778, 0.3982298685994925, 0.012972797822436946, -0.3761757585180234, -0.769255108356872, -1.1663049566789414, -1.5673654095295166]\n",
        "\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for action_prob, critic_value, ret in history:\n",
        "            # actor loss\n",
        "            diff = ret - critic_value\n",
        "            actor_losses.append(-action_prob * diff)\n",
        "\n",
        "            # critic loss\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(critic_value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # 전체 로스를 계산하고\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        # 로스에 대한 gradient를 계산\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        # 계산된 gradient로 모델을 업데이트\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # 히스토리들을 리셋\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # 1번의 에피소드를 진행했다. 카운트 증가\n",
        "    episode_count += 1\n",
        "\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} falls after {:.0f} steps at episode {}\"\n",
        "        print(template.format(running_reward, episode_reward, episode_count))\n",
        "\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYgTCTGf-GCP"
      },
      "source": [
        "## Visualizations\n",
        "In early stages of training:\n",
        "![Imgur](https://i.imgur.com/5gCs5kH.gif)\n",
        "\n",
        "In later stages of training:\n",
        "![Imgur](https://i.imgur.com/5ziiZUD.gif)\n"
      ]
    }
  ]
}